{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a6a744f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarvagra/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime, timezone \n",
    "from urllib.parse import urljoin, urlencode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fee0d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging configuration\n",
    "logging_dir=\"../logs\"\n",
    "os.makedirs(logging_dir, exist_ok=True)\n",
    "logging.basicConfig(filename=os.path.join(logging_dir,\"scraped_discourse_content.log\"), level=logging.DEBUG, format='%(asctime)s %(message)s', force=True)\n",
    "logging.info(\"logging is configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e678b9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration \n",
    "logging.info(\"setting up configuration variables ...\")\n",
    "try:\n",
    "    base_url = \"https://discourse.onlinedegree.iitm.ac.in/\"\n",
    "    category = \"courses/tds-kb\"\n",
    "    category_id = 34\n",
    "\n",
    "    # set start and end dates for topics to fetch\n",
    "    start_date = \"2025-01-01\" \n",
    "    end_date = \"2025-04-14\"   \n",
    "\n",
    "    cookie_str =\"_t=UhgFW7puAicWwzys5K9x4XXCeIbAiuOffLbEHteeJpOcxWknvKkpxkuCXAJDDj%2BCEBdIzV87gyPbeEKqYFM%2BTu4Nbus%2FZdJcHa9Ay9ME5NIKAq16wM4TaRgJK3Mo0cQsXXlGX2fdCT5D%2B5gbq%2FzE1oudFCFWqvejbRP9Lwr6gAuXOwvecipvP368M2ahMi7jBt8DW1M7g5AT75IMk4P9Pq2lgD4Wn2XFsNlDhSZC3a1bEBPd%2BHIh1GggvwXvG0kwR7SDQUqR26E9u8TK%2FO0UtXhvJPkGo1JIDK0t1agSkN%2BwlZmvtOKmRDGJsxZ%2FjnBR--efLSXFn%2BwaEvunu1--7%2B89YY7QWAMaRwNWi9a29Q%3D%3D; _forum_session=TtAj8x2p%2BWKovx0VoSopoqABmRXBQMMlCSjlvNY5Rda3Kllr4hE%2FwK9Dl8jwSmFcNhQlivTIOph3UZ3XTwi6duFXv2xDTJQmDFBPkIUmu187OPqj%2FtQ2hGXETtSR5VL8HJ1zDMFxoAvWmxo8gVsPkqCBI%2BcPaRV7ov5L7v%2FQiQUeRDRtKdJ0%2B9aRR0zw1qnnTYPFI7XOPXhnZjxGljbDlUAkCFH56QW5FoduR%2BK3IPGEbHjbcaISLF91OngGq9Swk%2FYuO9LZ9e2qRDItXzg0mEvZ01KGmQ%3D%3D--lBY22MvA7ZuSLszQ--QiC%2FbwHvWPWAH7XSYaiGEw%3D%3D\" \n",
    "    \n",
    "    out_dir = \"scraped_data_discourse_page\" \n",
    "    post_batch_id_size = 50 \n",
    "    breakpoint = 5 # New configuration for breaking loop\n",
    "    logging.info(\"configuration variables set successfully.\")\n",
    "\n",
    "except:\n",
    "    logging.error(\"Error in configuration. Please check the configuration variables.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a93ace0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse a raw cookie string as dictionary.\n",
    "\n",
    "logging.info(\"Parsing cookie string ...\")\n",
    "def parse_cookie_string(raw_cookie_string):\n",
    "    try:\n",
    "        cookies = {}\n",
    "        if not raw_cookie_string.strip():\n",
    "            print(\"Warning: RAW_COOKIE_STRING is empty. Requests might fail if authentication is needed.\")\n",
    "            return cookies\n",
    "        for cookie_part in raw_cookie_string.strip().split(\";\"):\n",
    "            if \"=\" in cookie_part:\n",
    "                key, value = cookie_part.strip().split(\"=\", 1)\n",
    "                cookies[key] = value\n",
    "        logging.info(\"cookie string parsed successfully.\")\n",
    "        return cookies\n",
    "    \n",
    "    except:\n",
    "        logging.error(\"error in parsing cookie string\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4c67ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetche topic IDs from a specific category within a date range.\n",
    "def get_topic_ids(base_url, category, category_id, start_date, end_date, cookies):\n",
    "    url = urljoin(base_url, f\"c/{category}/{category_id}.json\")\n",
    "    topic_ids = []\n",
    "    page = 0\n",
    "\n",
    "    start_dt_naive = datetime.fromisoformat(start_date + \"T00:00:00\")\n",
    "    start_dt = start_dt_naive.replace(tzinfo=timezone.utc)\n",
    "    end_dt_naive = datetime.fromisoformat(end_date + \"T23:59:59.999999\")\n",
    "    end_dt = end_dt_naive.replace(tzinfo=timezone.utc)\n",
    "\n",
    "    print(f\"Fetching topic IDs from category between {start_dt} and {end_dt}...\")\n",
    "\n",
    "    # Variables for the new loop break condition\n",
    "    consecutive_pages_with_no_new_unique_topics = 0\n",
    "    last_known_unique_topic_count = 0\n",
    "\n",
    "    while True:\n",
    "        paginated_url = f\"{url}?page={page}\"\n",
    "        try:\n",
    "            response = requests.get(paginated_url, cookies=cookies, timeout=30)\n",
    "            response.raise_for_status()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Failed to fetch page {page}: {e}\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            data = response.json()\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Failed to decode JSON from page {page}. Content: {response.text[:200]}...\")\n",
    "            break\n",
    "\n",
    "        topics_on_page = data.get(\"topic_list\", {}).get(\"topics\", [])\n",
    "\n",
    "        if not topics_on_page:\n",
    "            print(f\"No more topics found on page {page} (API returned empty list).\")\n",
    "            break # Primary stop condition: API says no more topics on this page\n",
    "\n",
    "        # Store current number of unique topics before processing this page\n",
    "        # This helps check if *this specific page fetch* added anything new\n",
    "        count_before_processing_page = len(set(topic_ids))\n",
    "\n",
    "        for topic in topics_on_page:\n",
    "            created_at_str = topic.get(\"created_at\")\n",
    "            if created_at_str:\n",
    "                try:\n",
    "                    created_date = datetime.fromisoformat(created_at_str.replace(\"Z\", \"+00:00\"))\n",
    "                except ValueError:\n",
    "                    print(f\"Warning: Could not parse date '{created_at_str}' for topic ID {topic.get('id')}\")\n",
    "                    continue\n",
    "\n",
    "                if start_dt <= created_date <= end_dt:\n",
    "                    topic_ids.append(topic[\"id\"]) # Add ID, will be deduped later for count\n",
    "\n",
    "        current_unique_topic_count = len(set(topic_ids))\n",
    "\n",
    "        if topics_on_page and current_unique_topic_count == count_before_processing_page :\n",
    "            # This means the current page had topics, but none of them were new *and* within the date range,\n",
    "            # or all topics fetched from this page were duplicates of ones already in topic_ids from *previous pages*.\n",
    "            # For the staleness check, we care if the overall unique set isn't growing.\n",
    "             pass # Handled by the check below using last_known_unique_topic_count\n",
    "\n",
    "        # Staleness check: Has the *total* number of unique topics found stopped growing?\n",
    "        if current_unique_topic_count == last_known_unique_topic_count and topics_on_page:\n",
    "            # topics_on_page is checked to ensure we don't increment if an empty page was returned (which is a valid end)\n",
    "            consecutive_pages_with_no_new_unique_topics += 1\n",
    "            print(f\"Page {page} did not yield any new unique topics. Consecutive stale pages: {consecutive_pages_with_no_new_unique_topics}.\")\n",
    "        else:\n",
    "            consecutive_pages_with_no_new_unique_topics = 0 # Reset if new unique topics were found\n",
    "\n",
    "        last_known_unique_topic_count = current_unique_topic_count\n",
    "\n",
    "        if consecutive_pages_with_no_new_unique_topics >= breakpoint:\n",
    "            print(f\"No new unique topics found for {breakpoint} consecutive pages. Assuming end of relevant category listing.\")\n",
    "            break\n",
    "\n",
    "        # Original secondary stop condition (heuristic)\n",
    "        more_topics_url = data.get(\"topic_list\", {}).get(\"more_topics_url\")\n",
    "        if not more_topics_url:\n",
    "            # This typically means it's the last page.\n",
    "            # The condition `len(topics_on_page) < 30` was a heuristic for when more_topics_url might be missing\n",
    "            # but the page wasn't full. If more_topics_url is definitively gone, it's a strong signal.\n",
    "            print(f\"No 'more_topics_url' indicated on page {page}. Assuming this is the last page of topics.\")\n",
    "            break\n",
    "        \n",
    "        print(f\"Fetched page {page}, {len(topics_on_page)} topics on page. Total unique topics found so far: {current_unique_topic_count}. Continuing...\")\n",
    "        page += 1\n",
    "\n",
    "\n",
    "    final_unique_topic_ids = list(set(topic_ids)) # Deduplicate\n",
    "    print(f\"Total unique topics found in timeframe: {len(final_unique_topic_ids)}\")\n",
    "    return final_unique_topic_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "934f2494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetches the full topic JSON, including all posts by handling pagination.\n",
    "def get_full_topic_json(base_url, topic_id, cookies):\n",
    "    initial_topic_url = urljoin(base_url, f\"t/{topic_id}.json\")\n",
    "    print(f\"Fetching initial data for topic {topic_id} from {initial_topic_url}\")\n",
    "\n",
    "    try:\n",
    "        response = requests.get(initial_topic_url, cookies=cookies, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        topic_data = response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to fetch initial topic data for {topic_id}: {e}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Failed to decode initial JSON for topic {topic_id}. Content: {response.text[:200]}...\")\n",
    "        return None\n",
    "\n",
    "    post_stream = topic_data.get(\"post_stream\")\n",
    "    if not post_stream or \"stream\" not in post_stream or \"posts\" not in post_stream:\n",
    "        print(f\"Error: 'post_stream' not found or incomplete in topic {topic_id}. Skipping post fetching.\")\n",
    "        return topic_data\n",
    "\n",
    "    all_post_ids_in_stream = post_stream.get(\"stream\", [])\n",
    "    loaded_post_ids = {post[\"id\"] for post in post_stream.get(\"posts\", [])}\n",
    "\n",
    "    all_post_ids_in_stream = [pid for pid in all_post_ids_in_stream if pid is not None]\n",
    "\n",
    "    missing_post_ids = [pid for pid in all_post_ids_in_stream if pid not in loaded_post_ids]\n",
    "\n",
    "    print(f\"Topic {topic_id}: Total posts in stream: {len(all_post_ids_in_stream)}, Initially loaded: {len(loaded_post_ids)}, Missing: {len(missing_post_ids)}\")\n",
    "\n",
    "    if not missing_post_ids:\n",
    "        print(f\"All posts for topic {topic_id} already loaded in initial fetch.\")\n",
    "        return topic_data\n",
    "\n",
    "    fetched_additional_posts = []\n",
    "    for i in range(0, len(missing_post_ids), post_batch_id_size):\n",
    "        batch_ids = missing_post_ids[i:i + post_batch_id_size]\n",
    "\n",
    "        query_params = [(\"post_ids[]\", pid) for pid in batch_ids]\n",
    "        posts_url = urljoin(base_url, f\"t/{topic_id}/posts.json\")\n",
    "\n",
    "        print(f\"Fetching batch of {len(batch_ids)} posts for topic {topic_id} (IDs: {batch_ids[0]}...{batch_ids[-1]})\")\n",
    "\n",
    "        try:\n",
    "            batch_response = requests.get(posts_url, params=query_params, cookies=cookies, timeout=60)\n",
    "            batch_response.raise_for_status()\n",
    "            batch_data = batch_response.json()\n",
    "\n",
    "            if isinstance(batch_data, list):\n",
    "                 fetched_additional_posts.extend(batch_data)\n",
    "            elif \"post_stream\" in batch_data and \"posts\" in batch_data[\"post_stream\"]:\n",
    "                fetched_additional_posts.extend(batch_data[\"post_stream\"][\"posts\"])\n",
    "            elif \"posts\" in batch_data and isinstance(batch_data[\"posts\"], list):\n",
    "                 fetched_additional_posts.extend(batch_data[\"posts\"])\n",
    "            else:\n",
    "                print(f\"Warning: Unexpected JSON structure for post batch in topic {topic_id}. Data: {str(batch_data)[:200]}...\")\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Failed to fetch post batch for topic {topic_id} (IDs: {batch_ids}): {e}\")\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Failed to decode JSON for post batch in topic {topic_id}. Response: {batch_response.text[:200]}...\")\n",
    "\n",
    "    if fetched_additional_posts:\n",
    "        print(f\"Successfully fetched {len(fetched_additional_posts)} additional posts for topic {topic_id}.\")\n",
    "        existing_posts_in_topic_data = {post['id']: post for post in topic_data[\"post_stream\"][\"posts\"]}\n",
    "        for post in fetched_additional_posts:\n",
    "            if post['id'] not in existing_posts_in_topic_data:\n",
    "                topic_data[\"post_stream\"][\"posts\"].append(post)\n",
    "                existing_posts_in_topic_data[post['id']] = post\n",
    "\n",
    "        post_id_to_post_map = {post['id']: post for post in topic_data[\"post_stream\"][\"posts\"]}\n",
    "\n",
    "        sorted_posts = []\n",
    "        for post_id_val in all_post_ids_in_stream: # Renamed post_id to post_id_val to avoid conflict\n",
    "            if post_id_val in post_id_to_post_map:\n",
    "                sorted_posts.append(post_id_to_post_map[post_id_val])\n",
    "\n",
    "        topic_data[\"post_stream\"][\"posts\"] = sorted_posts\n",
    "        print(f\"Topic {topic_id}: Final post count in JSON: {len(topic_data['post_stream']['posts'])}\")\n",
    "\n",
    "    return topic_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0498cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves the topic JSON data to a file.\n",
    "def save_topic_json(topic_id, json_data, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    filepath = os.path.join(output_dir, f\"topic_{topic_id}.json\")\n",
    "    try:\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(json_data, f, indent=2, ensure_ascii=False)\n",
    "        # print(f\"Successfully saved topic {topic_id} to {filepath}\") # Reduced verbosity\n",
    "    except IOError as e:\n",
    "        print(f\"Error saving topic {topic_id} to {filepath}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55713f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching topic IDs from category between 2025-01-01 00:00:00+00:00 and 2025-04-14 23:59:59.999999+00:00...\n",
      "Fetched page 0, 30 topics on page. Total unique topics found so far: 3. Continuing...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 48\u001b[0m\n\u001b[1;32m     44\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloaded files are in: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(out_dir))\n\u001b[1;32m     45\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScript finished.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 48\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cookies \u001b[38;5;129;01mand\u001b[39;00m base_url \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://meta.discourse.org/\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      6\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning without cookies. This may fail for private forums or specific content.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m topic_ids \u001b[38;5;241m=\u001b[39m \u001b[43mget_topic_ids\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategory_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcookies\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m topic_ids:\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo topic IDs found for the given criteria. Exiting.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 21\u001b[0m, in \u001b[0;36mget_topic_ids\u001b[0;34m(base_url, category, category_id, start_date, end_date, cookies)\u001b[0m\n\u001b[1;32m     19\u001b[0m paginated_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m?page=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 21\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaginated_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcookies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcookies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/urllib3/connectionpool.py:464\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 464\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    466\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mconn\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/urllib3/connectionpool.py:1093\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m-> 1093\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mproxy_is_verified:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/urllib3/connection.py:704\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[0;32m--> 704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m     server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n\u001b[1;32m    706\u001b[0m     tls_in_tls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/urllib3/connection.py:198\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Establish a socket connection and set nodelay settings on it.\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m:return: New socket connection.\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 198\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/urllib3/util/connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source_address:\n\u001b[1;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 73\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n\u001b[1;32m     75\u001b[0m err \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to orchestrate the downloading process.\"\"\"\n",
    "    logging.info(\"script started\")\n",
    "    cookies = parse_cookie_string(cookie_str)\n",
    "    if not cookies and base_url != \"https://meta.discourse.org/\":\n",
    "        logging.warning(\"Running without cookies. This may fail for private forums or specific content.\")\n",
    "\n",
    "    topic_ids = get_topic_ids(\n",
    "        base_url,\n",
    "        category,\n",
    "        category_id,\n",
    "        start_date,\n",
    "        end_date,\n",
    "        cookies\n",
    "    )\n",
    "\n",
    "    if not topic_ids:\n",
    "        print(\"No topic IDs found for the given criteria. Exiting.\")\n",
    "        return\n",
    "\n",
    "    total_topics = len(topic_ids)\n",
    "    success_downloads = 0\n",
    "    failed_topic_ids = []\n",
    "\n",
    "    print(f\"\\nStarting download of {total_topics} topics...\\n\")\n",
    "\n",
    "    for i, topic_id in enumerate(topic_ids, 1):\n",
    "        print(f\"--- [{i}/{total_topics}] Processing topic ID: {topic_id} ---\")\n",
    "        topic_json_data = get_full_topic_json(base_url, topic_id, cookies)\n",
    "        if topic_json_data:\n",
    "            save_topic_json(topic_id, topic_json_data, out_dir)\n",
    "            success_downloads += 1\n",
    "        else:\n",
    "            logging.warning(\"Failed to get complete data for topic %d.\", topic_id)\n",
    "            failed_topic_ids.append(topic_id)\n",
    "        # logging.info(\"--- Finished processing topic ID: %d ---\", topic_id)\n",
    "\n",
    "    logging.info(\"\\n========= SUMMARY =========\")\n",
    "    logging.info(f\"Total topics identified: {total_topics}\")\n",
    "    logging.info(f\"Successfully downloaded full data for: {success_downloads} topics\")\n",
    "    logging.info(f\"Failed to download/process: {len(failed_topic_ids)} topics\")\n",
    "    if failed_topic_ids:\n",
    "        logging.info(\"Failed topic IDs: %s\", failed_topic_ids)\n",
    "    logging.info(\"Downloaded files are in: %s\", os.path.abspath(out_dir))\n",
    "    logging.info(\"Script finished.\")\n",
    "\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
