{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "308779d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing files :\n",
    "import logging \n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "from urllib.parse import urljoin\n",
    "from markdownify import markdownify as md\n",
    "from playwright.async_api import async_playwright\n",
    "import asyncio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a5ea69d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://tds.s-anand.net/#/2025-01/\"\n",
    "BASE_ORIGIN = \"https://tds.s-anand.net\"\n",
    "OUTPUT_DIR = \"scraped_data_course_content\"\n",
    "METADATA_FILE = \"metadata.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0defe01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "\n",
    "logging_dir=\"logs\"\n",
    "os.makedirs(logging_dir, exist_ok=True)\n",
    "logging.basicConfig(filename=os.path.join(logging_dir,\"scraped_course_content.log\"), level=logging.DEBUG, format='%(asctime)s %(message)s', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea7ee196",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "visited = set()\n",
    "metadata = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "78c7c098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to format the file name\n",
    "def format_filename(title):\n",
    "    return re.sub(r'[\\\\/*?:\"<>|]', \"_\", title).strip().replace(\" \", \"_\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d1e63a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract all internal links \n",
    "async def extract_internal_links(page):\n",
    "    links = await page.eval_on_selector_all(\"a[href]\", \"els => els.map(el => el.href)\")\n",
    "    return list(set(\n",
    "        link for link in links\n",
    "        if BASE_ORIGIN in link and '/#/' in link\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d6a72929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape a page\n",
    "async def wait_for_article_and_get_html(page):\n",
    "    await page.wait_for_selector(\"article.markdown-section#main\", timeout=10000)\n",
    "    return await page.inner_html(\"article.markdown-section#main\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "815eeb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goto the base URL and start crawling\n",
    "async def crawl_page(page, url):\n",
    "    if url in visited:\n",
    "        return\n",
    "    visited.add(url)\n",
    "\n",
    "    logging.info(f\"üìÑ Visiting: {url}\")\n",
    "    try:\n",
    "        await page.goto(url, wait_until=\"domcontentloaded\")\n",
    "        await page.wait_for_timeout(1000)\n",
    "        html = await wait_for_article_and_get_html(page)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"‚ùå Error loading page: {url}\\n{e}\")\n",
    "        return\n",
    "\n",
    "    # Extract title and save markdown\n",
    "    title = (await page.title()).split(\" - \")[0].strip() or f\"page_{len(visited)}\"\n",
    "    filename = format_filename(title)\n",
    "    filepath = os.path.join(OUTPUT_DIR, f\"{filename}.md\")\n",
    "\n",
    "    markdown = md(html)\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"---\\n\")\n",
    "        f.write(f\"title: \\\"{title}\\\"\\n\")\n",
    "        f.write(f\"original_url: \\\"{url}\\\"\\n\")\n",
    "        f.write(f\"downloaded_at: \\\"{datetime.now().isoformat()}\\\"\\n\")\n",
    "        f.write(f\"---\\n\\n\")\n",
    "        f.write(markdown)\n",
    "\n",
    "    metadata.append({\n",
    "        \"title\": title,\n",
    "        \"filename\": f\"{filename}.md\",\n",
    "        \"original_url\": url,\n",
    "        \"downloaded_at\": datetime.now().isoformat()\n",
    "    })\n",
    "\n",
    "    # Recursively crawl all links found on the page (not just main content)\n",
    "    links = await extract_internal_links(page)\n",
    "    for link in links:\n",
    "        if link not in visited:\n",
    "            await crawl_page(page, link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "531df1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main method \n",
    "async def main():\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    global visited, metadata\n",
    "\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        context = await browser.new_context()\n",
    "        page = await context.new_page()\n",
    "        await crawl_page(page, BASE_URL)\n",
    "\n",
    "        with open(METADATA_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "\n",
    "        logging.info(f\"\\n‚úÖ Completed. {len(metadata)} pages saved.\")\n",
    "        await browser.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79499f4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
